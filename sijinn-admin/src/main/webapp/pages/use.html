
 <!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>SIJINN</title>
	<link rel="SHORTCUT ICON" href="images/application.ico">



    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Custom styles for this template -->
    <style>
      body {
        padding-top: 20px;
        padding-bottom: 20px;
      }

      .navbar {
        margin-bottom: 20px;
      }
    </style>
    
    <!-- Styles for avoiding jQuery -->
    <link rel="stylesheet" href="css/navbar.css">

  </head>

  <body>

	<div class="container" style="padding-top: 50px">
      <!-- Navbar -->
		<nav id="top_menu" class="navbar navbar-default navbar-fixed-top"></nav>
      <!-- Main component for a primary marketing message or call to action -->
      	<div class="jumbotron" style="padding:5px;background-color: white;">

     		<div class="col-sm-9" data-spy="scroll" data-target="#nav-scrollspy">
				<section id="install">
					<h2>Install</h2>
					<h3>Download from Sourse Forge</h3> 
					<h5>
						The latest version of the library SIJINN is available for download on the <a href="https://sourceforge.net/projects/sijinn/files/v.1.1.2/">SorceForge repository as sijinn-base-[version].jar</a>.
						For correct processing It will be also necessary to download the package of <a href="https://logging.apache.org/log4j/2.0/download.html">Apache Log4j</a> (version 2.5) from the official site.<br>
						After download you will can integrate this library into your development/production environment.
					</h5>
					<h3>Maven Integration</h3>
<pre ><xmp><repositories>
	<repository>
		<id>neohort-mvn-repo</id> 
		<url>https://github.com/surban1974/sijinn/raw/mvn-repo/</url>
		<snapshots>
			<enabled>true</enabled>
			<updatePolicy>always</updatePolicy>
		</snapshots>
	</repository>
</repositories>
						
<dependency>
	<groupId>com.github.surban1974.sijinn</groupId>
	<artifactId>sijinn-base</artifactId>
	<version>1.1.2-alfa</version>
</dependency>					
</xmp></pre> 

					<h3>Sources</h3>
					<h5>
						The source code of SIJINN can be found into the <a href="https://github.com/surban1974/sijinn/tree/master/sijinn"> GitHub repository </a>
					</h5>
				</section>	
				<br>
				<section id="examples">
				
					<h2>How to</h2>
					
					<h3 id="exampleCreateNetwork">Create simple perceptron</h3>
					<h5>
						with 3 layers when the first layer (input) contain 2 neurons, the second (hidden) - 4 neurons, the last (output) - 1 neuron. 
						As activation function is used Sigmoid functional.
					</h5>
					<h5><b>mode 1 - with constructor</b></h5>
					<pre><xmp>
final Network network = new Network(
	new ArrayList<List<Neuron>>(
		Arrays.asList(
			Network.createLayer(2, new SimpleSigmoidFermi()),
			Network.createLayer(4, new SimpleSigmoidFermi()),
			Network.createLayer(1, new SimpleSigmoidFermi())
			)
		),
		new RandomPositiveWeightGenerator()
	);					
					</xmp></pre>					
					<h5><b>mode 2 - add layers</b></h5>		
					<pre><xmp>
final Network network = new Network()
	.addLayer(2, new SimpleSigmoidFermi())
	.addLayer(4, new SimpleSigmoidFermi())
	.addLayer(1, new SimpleSigmoidFermi())
	.createSynapses(new RandomPositiveWeightGenerator());					
					</xmp></pre>	
					
					<h5><b>mode 3 - add neurons</b></h5>
					<pre><xmp>					
final Network network = new Network();
	network
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),0,0), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),0,1), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,0), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,1), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,2), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,3), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),2,0), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,1), false)
	.createSynapses(new RandomPositiveWeightGenerator());			
					</xmp></pre>

					<h5><b>mode 4 - add neurons and synapses</b></h5>
					<pre><xmp>					
final Network network = new Network();
	network
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),0,0), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),0,1), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,0), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,1), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,2), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,3), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),2,0), false)
	.addNeuron(new Neuron(network,new SimpleSigmoidFermi(),1,1), false)
	.addSynapse(new Synapse(0, 0, 1, 0, new RandomPositiveWeightGenerator()), false)
	.addSynapse(new Synapse(0, 0, 1, 1, new RandomPositiveWeightGenerator()), false)
	.addSynapse(new Synapse(0, 0, 1, 2, new RandomPositiveWeightGenerator()), false)
	...
	.addSynapse(new Synapse(1, 3, 2, 1, new RandomPositiveWeightGenerator()), false);			
					</xmp></pre>	
					
					
					<br>
					<h3 id="exampleCreateAlgorithm">Create training algorithm</h3>
					<h5><b>Back propagation</b></h5>
					<pre><xmp>	
ITrainingAlgorithm trainingAlgorith = 
	new BPROP()
	.setLearningRate(learningRate)
	.setLearningMomentum(learningMomentum)
	.setDeferredAgregateFunction(new SUMMATOR());
					</xmp></pre>
					
					<h5><b>Quick propagation</b></h5>
					<pre><xmp>	
ITrainingAlgorithm trainingAlgorith = 
	new QPROP()
	.setLearningRate(learningRate)
	.setDeferredAgregateFunction(new SUMMATOR());
					</xmp></pre>					
					
					<h5><b>Resilient propagation</b></h5>
					<pre><xmp>	
ITrainingAlgorithm trainingAlgorith = 
	new RPROP().
		setInitialDeltaGenarator(
			new IGenerator() {			
				@Override
				public float generate(Neuron from, Neuron to) {
					return 0.1f;
				}
			})
			.setDeferredAgregateFunction( new SUMMATOR());
					</xmp></pre>	
									
					<h5><b>Genetic</b></h5>
					<pre><xmp>	
ITrainingAlgorithm trainingAlgorith = 
	new GENE(
		new NeuralBreeding().setElitism(false)
	)
	.setPopulationSize(50);
					</xmp></pre>										
					
					<br>
					<h3 id="exampleCreateStrategy">Create training strategy</h3>	
					<h5><b>Stochastic Gradient Descent (with back propagation)</b></h5>
					<pre><xmp>	
final ITrainingStrategy trainingStrategy = new StochasticGradientDescent(
	new BPROP()
		.setLearningRate(learningRate)
		.setLearningMomentum(learningMomentum)
	)
	.setErrorFunction(new MSE());
					</xmp></pre>

					<h5><b>Online Gradient Descent (with back propagation)</b></h5>
					<pre><xmp>	
final ITrainingStrategy trainingStrategy = new OnlineGradientDescent(
	new BPROP()
		.setLearningRate(learningRate)
		.setLearningMomentum(learningMomentum)
	)
	.setErrorFunction(new MSE());
					</xmp></pre>

					<h5><b>Batch Gradient Descent (with resilient backpropagation)</b></h5>
					<pre><xmp>	
final ITrainingStrategy trainingStrategy = new BatchGradientDescent(
	new RPROP()
		.setInitialDeltaGenarator(
			new IGenerator() {			
				@Override
				public float generate(Neuron from, Neuron to) {
					return 0.1f;
				}
			})
		.setDeferredAgregateFunction( new SUMMATOR())
		)
		.setErrorFunction(new MSE()); 
					</xmp></pre>

					<h5><b>Genetic strategy (with genetic algorithm)</b></h5>
					<pre><xmp>	
final ITrainingStrategy trainingStrategy = new GeneticBreeding(
	new GENE(
		new NeuralBreeding().setElitism(false)
		)
		.setPopulationSize(50)
	)				
	.setErrorFunction(new MSE());
					</xmp></pre>
					
					<br>
					<h3 id="exampleTraining">Training</h3>	
					<h5><b>1.1. Create training data reader (suppose the training data is resource into project)</b></h5>
					<pre><xmp>	
final String resource_training = "examples/resources/interpolation_training.txt";
final IStreamWrapper dataReader = new ResourceStreamWrapper(resource_training);
final IReadLinesAggregator aggregator = new SimpleLineDataAggregator(";");
					</xmp></pre>
					<h5><b>1.2. Create training data reader (suppose the training data is an external file)</b></h5>
					<pre><xmp>	
final File resource_training = new File("c:/training.dat");
final IDataReader dataReader = new SimpleFileReader(resource_training);
final IReadLinesAggregator aggregator = new SimpleLineDataAggregator(";");
					</xmp></pre>					
					<h5><b>1.3. Create training data reader (suppose the training data is an array of float)</b></h5>
					<pre><xmp>	
final float[][][] trainingData = 					
				new float[][][] {						
					{{1, 1}, {0}},
			        {{0, 1}, {1}},
			        {{1, 0}, {1}},
			        {{0, 0}, {0}},
				};
final IDataReader dataReader = new SimpleArrayReader(trainingData);
final IReadLinesAggregator aggregator = new SimpleArrayDataAggregator();
					</xmp></pre>	
					
					<h5><b>2. Processing</b></h5>
					<pre><xmp>
final float approximation = 0.001f;					
float delta = 100000;						
while(delta>approximation){
	delta = network.training(
				dataReader,
				trainingStrategy,
				aggregator);
}				
					</xmp></pre>
					
					<h5><b>3. Save/read trained network</b></h5>
					<pre><xmp>
network.save("c:/network.net");
...
network = new Network().open(new File("c:/network.net"));	
					</xmp></pre>	
					
					<br>
					<h3 id="exampleTest">Test network</h3>	
					<pre><xmp>	
final String resource_test = "examples/resources/interpolation_test.txt";
final IStreamWrapper testDataReader = new ResourceStreamWrapper(resource_test);
final IReadLinesAggregator testAggregator = new SimpleLineDataAggregator(";");
float error_test = network.test(testDataReader, testAggregator, new MSE());
					</xmp></pre>					
																																				
				</section>

				
 
         
      		</div>
       	
 				
     		<div id="side-nav" class="col-sm-3">
				<h3 class="visible-xs">Navigation</h3>
				<ul id="nav-scrollspy" class="nav nav-stacked" data-spy="affix" data-target="#install" data-offset-bottom="200">
					<li><a href="#install">Install</a></li>
					<li>
						<a href="#examples">How to...</a>
						<ul class="nav nav-stacked">
							<li><a href="#exampleCreateNetwork">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;create network</a></li>
							<li><a href="#exampleCreateAlgorithm">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;create training algorithm</a></li>
							<li><a href="#exampleCreateStrategy">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;create training strategy</a></li>
							<li><a href="#exampleTraining">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;training network</a></li>
							<li><a href="#exampleTest">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test network</a></li>

						</ul>
					</li>
				</ul>
			</div>
    	
      	
      	
      	
      	</div>	

	</div> <!-- /container -->


    <!-- Navbar JavaScript -->
    <script src="js/bootstrap-native.patched.js"></script>
	
	<!--  ClassHidra Ajax JavaScript -->
	<script src='js/clAjax.js'></script>  	 
	
	
	<script>
		new clajax()
			.setUrl("pages/included/top_menu.html")
			.setTarget(document.getElementById("top_menu"))
			.setSuccess(function(){ 
				document.getElementById("top_menu_use").className += " active";
			})
			.request("POST");
	
	</script>
  </body>
</html>